{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "# import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.integrate import nquad, quad, simps\n",
    "from scipy.stats import entropy, gamma, multivariate_normal, norm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sktree.ensemble import HonestForestClassifier\n",
    "from sktree.stats import build_hyppo_oob_forest\n",
    "from sktree.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_at_specificity(y_true, y_score, target_specificity=0.98, pos_label=1):\n",
    "    n_trees, n_samples, n_classes = y_score.shape\n",
    "\n",
    "    # Compute nan-averaged y_score along the trees axis\n",
    "    y_score_avg = np.nanmean(y_score, axis=0)\n",
    "\n",
    "    # Extract true labels and nan-averaged predicted scores for the positive class\n",
    "    y_true = y_true.ravel()\n",
    "    y_score_binary = y_score_avg[:, 1]\n",
    "\n",
    "    # Identify rows with NaN values in y_score_binary\n",
    "    nan_rows = np.isnan(y_score_binary)\n",
    "\n",
    "    # Remove NaN rows from y_score_binary and y_true\n",
    "    y_score_binary = y_score_binary[~nan_rows]\n",
    "    y_true = y_true[~nan_rows]\n",
    "\n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score_binary, pos_label=pos_label)\n",
    "\n",
    "    # Find the threshold corresponding to the target specificity\n",
    "    index = np.argmax(fpr >= (1 - target_specificity))\n",
    "    threshold_at_specificity = thresholds[index]\n",
    "\n",
    "    # Compute sensitivity at the chosen specificity\n",
    "    # sensitivity = tpr[index]\n",
    "    # return sensitivity\n",
    "\n",
    "    # Use the threshold to classify predictions\n",
    "    y_pred_at_specificity = (y_score_binary >= threshold_at_specificity).astype(int)\n",
    "\n",
    "    # Compute sensitivity at the chosen specificity\n",
    "    sensitivity = np.sum((y_pred_at_specificity == 1) & (y_true == 1)) / np.sum(\n",
    "        y_true == 1\n",
    "    )\n",
    "\n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_PATH = '/Users/baiyuxin/Desktop/JHU/NDD/Cancer/mendseqs/'\n",
    "# FILE = [\"alufraction.csv.pkl\",\"mendseq.featurematrix.csv.pkl\",\"MendSeqS_Length.featurematrix.csv.pkl\",\"wps.featurematrix.csv.pkl\"]\n",
    "FILE_PATH ='/Users/baiyuxin/Desktop/JHU/NDD/Cancer/MIGHT_TEST_v0.6.1/alus_ratio.csv'\n",
    "N_ESTIMATORS = 1000\n",
    "# N_ESTIMATORS[0] = 10\n",
    "REPS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Experiment                  Sample  Cancer Status  Tumor type Stage  \\\n",
      "0         S0028   S0028.INDI_918_PLS_1A              1     Stomach    IV   \n",
      "1         S0028    S0028.INDI_980_PLS_1              1     Stomach    IV   \n",
      "2         S0034   S0034.INDI_580_PLS_1A              1  Colorectal    IV   \n",
      "3         S0034   S0034.INDI_730_PLS_1A              1    Pancreas    IV   \n",
      "4         S0034   S0034.INDI_481_PLS_1A              1       Liver    IV   \n",
      "...         ...                     ...            ...         ...   ...   \n",
      "1987      S0294  S0294.INDIA_3493_PLS_1              1     Stomach   IIB   \n",
      "1988      S0294  S0294.INDIA_3494_PLS_1              1     Stomach  IIIA   \n",
      "1989      S0294  S0294.INDIA_3496_PLS_1              1     Stomach    IA   \n",
      "1990      S0294  S0294.INDIA_3500_PLS_1              1     Stomach   IIA   \n",
      "1991      S0294  S0294.INDIA_3482_PLS_1              1     Stomach    IB   \n",
      "\n",
      "          AluY      AluS      AluJ  \n",
      "0     0.120170  0.611256  0.268575  \n",
      "1     0.120447  0.612803  0.266749  \n",
      "2     0.117988  0.609682  0.272330  \n",
      "3     0.119883  0.612847  0.267270  \n",
      "4     0.126447  0.606821  0.266732  \n",
      "...        ...       ...       ...  \n",
      "1987  0.122200  0.616676  0.261125  \n",
      "1988  0.120226  0.616831  0.262943  \n",
      "1989  0.120798  0.618799  0.260403  \n",
      "1990  0.119772  0.616931  0.263297  \n",
      "1991  0.120376  0.617862  0.261762  \n",
      "\n",
      "[1992 rows x 8 columns]\n",
      "<_io.TextIOWrapper name='/Users/baiyuxin/Desktop/JHU/NDD/Cancer/MIGHT_TEST_v0.6.1/Cohort1.samples.txt' mode='r' encoding='UTF-8'>\n",
      "['S0028.INDI_980_PLS_1', 'S0034.INDI_580_PLS_1A', 'S0034.INDI_730_PLS_1A', 'S0034.INDI_481_PLS_1A', 'S0034.INDI_193_PLS_1A', 'S0035.INDI_020_PLS_1A', 'S0035.INDI_998_PLS_1', 'S0035.INDI_983_PLS_1', 'S0035.INDI_965_PLS_1', 'S0035.INDI_510_PLS_1A', 'S0035.INDI_509_PLS_1A', 'S0075.INDIA_3379_PLS_1', 'S0076.INDIA_3398_PLS_1', 'S0076.INDIA_3392_PLS_1', 'S0076.INDIA_3399_PLS_1', 'S0080.INDI_416_PLS_1A', 'S0080.INDI_330_PLS_1A', 'S0080.INDI_895_PLS_1A', 'S0080.INDI_804_PLS_1A', 'S0080.INDI_744_PLS_1A', 'S0080.INDI_542_PLS_1A', 'S0123B.PANCA_1267_PLS_01', 'S0129A.INDIA_2178_PLS_1', 'S0129A.INDIA_2182_PLS_1', 'S0129A.INDIA_2181_PLS_1', 'S0129B.INDIA_3742_PLS_1', 'S0129B.INDIA_3732_PLS_1', 'S0129B.INDIA_3733_PLS_1', 'S0129B.INDIA_3734_PLS_1', 'S0131.INDIA_2161_PLS_1', 'S0131.INDIA_2143_PLS_1', 'S0131.INDIA_3752_PLS_1', 'S0131.INDIA_3745_PLS_1', 'S0131.INDIA_3747_PLS_1', 'S0131.INDIA_3750_PLS_1', 'S0131B.INDIA_3754_PLS_1', 'S0131B.INDIA_3756_PLS_1', 'S0133A.INDIA_2190_PLS_1', 'S0133A.INDIA_2171_PLS_1', 'S0133A.INDIA_2194_PLS_1', 'S0133A.INDIA_2192_PLS_1', 'S0133A.INDIA_2191_PLS_1', 'S0133A.INDIA_2186_PLS_1', 'S0133A.INDIA_2195_PLS_1', 'S0133A.INDIA_2166_PLS_1', 'S0133B.INDIA_2201_PLS_1', 'S0133B.INDIA_2200_PLS_1', 'S0134A.INDIA_2641_PLS_1', 'S0134A.INDIA_2635_PLS_1', 'S0134A.INDIA_2631_PLS_1', 'S0134A.INDIA_2628_PLS_1', 'S0134B.INDIA_2648_PLS_1', 'S0134B.INDIA_2650_PLS_1', 'S0136A.INDIA_2120_PLS_1', 'S0136B.PANCA_1263_PLS_01', 'S0139.INDIA_3474_PLS_1', 'S0145.INDIA_2914_PLS_1', 'S0145.INDIA_2919_PLS_1', 'S0145.INDIA_2921_PLS_1', 'S0149.INDIA_2822_PLS_1', 'S0149.INDIA_2821_PLS_1', 'S0149.INDIA_2820_PLS_1', 'S0149.INDIA_2819_PLS_1', 'S0149.INDIA_2799_PLS_1', 'S0149.INDIA_3034_PLS_1', 'S0149.INDIA_2823_PLS_1', 'S0149.INDIA_2784_PLS_1', 'S0149.INDIA_2824_PLS_1', 'S0149.INDIA_2866_PLS_1', 'S0149.INDIA_2864_PLS_1', 'S0149.INDIA_2865_PLS_1', 'S0149.INDIA_3035_PLS_1', 'S0149.INDIA_3038_PLS_1', 'S0149.INDIA_2724_PLS_1', 'S0149.INDIA_303_PLS_17', 'S0149.INDIA_2825_PLS_1', 'S0149.INDIA_2380_PLS_1', 'S0149.INDIA_3032_PLS_1', 'S0149.INDIA_3033_PLS_1', 'S0155.INDIA_2682_PLS_1', 'S0155.INDIA_2681_PLS_1', 'S0156.INDIA_3882_PLS_1', 'S0156.INDIA_2741_PLS_1', 'S0156.INDIA_2737_PLS_1', 'S0156.INDIA_2734_PLS_1', 'S0156.INDIA_2740_PLS_1', 'S0157.INDIA_2961_PLS_1', 'S0157.INDIA_2956_PLS_1', 'S0157.INDIA_2957_PLS_1', 'S0157.INDIA_2958_PLS_1', 'S0157.INDIA_2959_PLS_1', 'S0157.INDIA_3911_PLS_1', 'S0157.INDIA_2952_PLS_1', 'S0157.INDIA_3887_PLS_1', 'S0157.INDIA_3900_PLS_1', 'S0157.INDIA_3899_PLS_1', 'S0157.INDIA_3907_PLS_1', 'S0157.INDIA_3906_PLS_1', 'S0158.INDIA_2690_PLS_1', 'S0158.INDIA_2686_PLS_1', 'S0159.INDIA_2729_PLS_1', 'S0159.INDIA_2731_PLS_1', 'S0159.INDIA_2727_PLS_1', 'S0159.INDIA_2376_PLS_1', 'S0159.INDIA_2372_PLS_1', 'S0159.INDIA_2733_PLS_1', 'S0160.INDIA_3790_PLS_1', 'S0160.INDIA_2723_PLS_1', 'S0160.INDIA_2719_PLS_1', 'S0161.INDIA_2699_PLS_1', 'S0161.INDIA_2692_PLS_1', 'S0161.INDIA_2693_PLS_1', 'S0161.INDIA_2696_PLS_1', 'S0161.INDIA_2704_PLS_1', 'S0161.INDIA_2710_PLS_1', 'S0161.INDIA_2711_PLS_1', 'S0161.INDIA_2712_PLS_1', 'S0161.INDIA_2714_PLS_1', 'S0161.INDIA_2715_PLS_1', 'S0161.INDIA_2708_PLS_1', 'S0162.INDIA_2755_PLS_1', 'S0162.INDIA_2748_PLS_1', 'S0162.INDIA_2750_PLS_1', 'S0162.INDIA_2751_PLS_1', 'S0162.INDIA_2747_PLS_1', 'S0162.INDIA_2756_PLS_1', 'S0162.INDIA_2752_PLS_1', 'S0164.INDIA_2763_PLS_1', 'S0164.INDIA_2761_PLS_1', 'S0164.INDIA_2760_PLS_1', 'S0166.INDIA_2766_PLS_1', 'S0166.INDIA_2767_PLS_1', 'S0167.INDIA_2426_PLS_1', 'S0167.INDIA_2408_PLS_1', 'S0167.INDIA_2412_PLS_1', 'S0167.INDIA_2406_PLS_1', 'S0167.INDIA_2405_PLS_1', 'S0167.INDIA_2402_PLS_1', 'S0167.INDIA_2428_PLS_1', 'S0167.INDIA_2429_PLS_1', 'S0167.INDIA_2417_PLS_1', 'S0167.INDIA_2419_PLS_1', 'S0167.INDIA_2420_PLS_1', 'S0167.INDIA_2421_PLS_1', 'S0167.INDIA_2423_PLS_1', 'S0167.INDIA_2425_PLS_1', 'S0167.INDIA_2414_PLS_1', 'S0168.INDIA_2772_PLS_1', 'S0168.INDIA_2774_PLS_1', 'S0168.INDIA_2778_PLS_1', 'S0168.INDIA_3845_PLS_1', 'S0173.INDIA_2872_PLS_1', 'S0173.INDIA_2851_PLS_1', 'S0173.INDIA_2854_PLS_1', 'S0173.INDIA_2857_PLS_1', 'S0174.INDIA_2441_PLS_1', 'S0174.INDIA_2435_PLS_1', 'S0175.INDIA_2797_PLS_1', 'S0175.INDIA_2809_PLS_1', 'S0175.INDIA_2802_PLS_1', 'S0175.INDIA_2812_PLS_1', 'S0176.INDIA_3912_PLS_1', 'S0176.INDIA_3898_PLS_1', 'S0176.INDIA_2796_PLS_1', 'S0176.INDIA_279_PLS_13', 'S0176.INDIA_3893_PLS_1', 'S0176.INDIA_2795_PLS_1', 'S0176.INDIA_2789_PLS_1', 'S0176.INDIA_2781_PLS_1', 'S0177.INDIA_3932_PLS_1', 'S0177.INDIA_3927_PLS_1', 'S0177.INDIA_2965_PLS_1', 'S0177.INDIA_2816_PLS_1', 'S0178.INDIA_2675_PLS_1', 'S0178.INDIA_2679_PLS_1', 'S0178.INDIA_2680_PLS_1', 'S0178.INDIA_2652_PLS_1', 'S0179.INDIA_3933_PLS_1', 'S0179.INDIA_2831_PLS_1', 'S0179.INDIA_3854_PLS_1', 'S0179.INDIA_3807_PLS_1', 'S0180.INDIA_3860_PLS_1', 'S0180.INDIA_3858_PLS_1', 'S0180.INDIA_2849_PLS_1', 'S0180.INDIA_2848_PLS_1', 'S0180.INDIA_2847_PLS_1', 'S0180.INDIA_3958_PLS_1', 'S0181.INDIA_2884_PLS_1', 'S0181.INDIA_2885_PLS_1', 'S0181.INDIA_2882_PLS_1', 'S0181.INDIA_4027_PLS_1', 'S0182.INDIA_2894_PLS_1', 'S0182.INDIA_2895_PLS_1', 'S0182.INDIA_2896_PLS_1', 'S0182.INDIA_2893_PLS_1', 'S0182.INDIA_2899_PLS_1', 'S0182.INDIA_2399_PLS_1', 'S0182.INDIA_2397_PLS_1', 'S0182.INDIA_2398_PLS_1', 'S0185.INDIA_2945_PLS_1', 'S0185.INDIA_2941_PLS_1', 'S0185.INDIA_2936_PLS_1', 'S0185.INDIA_2932_PLS_1', 'S0185.INDIA_2939_PLS_1', 'S0186.INDIA_4032_PLS_1', 'S0186.INDIA_2971_PLS_1', 'S0187.INDIA_3998_PLS_1', 'S0187.INDIA_4037_PLS_1', 'S0187.INDIA_4021_PLS_1', 'S0187.INDIA_2981_PLS_1', 'S0187.INDIA_2979_PLS_1', 'S0187.INDIA_2977_PLS_1', 'S0187.INDIA_2975_PLS_1', 'S0188.INDIA_3022_PLS_1', 'S0188.INDIA_3021_PLS_1', 'S0188.INDIA_3025_PLS_1', 'S0188.INDIA_4004_PLS_1', 'S0188.INDIA_3017_PLS_1', 'S0188.INDIA_3015_PLS_1', 'S0188.INDIA_3010_PLS_1', 'S0188.INDIA_4012_PLS_1', 'S0188.INDIA_3018_PLS_1', 'S0189.INDIA_3027_PLS_1', 'S0189.INDIA_3028_PLS_1', 'S0189.INDIA_3031_PLS_1', 'S0189.INDIA_2948_PLS_1', 'S0189.INDIA_2949_PLS_1', 'S0190.INDIA_2456_PLS_1', 'S0190.INDIA_2459_PLS_1', 'S0190.INDIA_2951_PLS_1', 'S0190.INDIA_2453_PLS_1', 'S0190.INDIA_2454_PLS_1', 'S0190.INDIA_2878_PLS_1', 'S0190.INDIA_2877_PLS_1', 'S0193.INDIA_2471_PLS_1', 'S0193.INDIA_2469_PLS_1', 'S0193.INDIA_2468_PLS_1', 'S0193.INDIA_2472_PLS_1', 'S0193.INDIA_2996_PLS_1', 'S0193.INDIA_2991_PLS_1', 'S0193.INDIA_2997_PLS_1', 'S0193.INDIA_2992_PLS_1', 'S0193.INDIA_2995_PLS_1', 'S0197.INDIA_2465_PLS_1', 'S0197.INDIA_2473_PLS_1', 'S0197.INDIA_3007_PLS_1', 'S0199.INDIA_2491_PLS_1', 'S0199.INDIA_2489_PLS_1', 'S0199.INDIA_2484_PLS_1', 'S0199.INDIA_2480_PLS_1', 'S0199.INDIA_2478_PLS_1', 'S0199.INDIA_2495_PLS_1', 'S0199.INDIA_2496_PLS_1', 'S0199.INDIA_2493_PLS_1', 'S0200.INDIA_2504_PLS_1', 'S0202.INDIA_3000_PLS_1', 'S0209.INDI_845_PLS_1A', 'S0209.INDI_689_PLS_1A', 'S0210.INDI_435_PLS_1A', 'S0215.INDIA_4131_PLS_1', 'S0215.INDIA_4132_PLS_1', 'S0215.INDIA_4141_PLS_1', 'S0215.INDIA_4133_PLS_1', 'S0215.INDIA_4135_PLS_1', 'S0216.NL_PLSA_2425', 'S0216.NL_PLSA_2426', 'S0216.NL_PLSA_2431', 'S0216.NL_PLSA_2428', 'S0216.NL_PLSA_2453', 'S0216.NL_PLSA_2452', 'S0216.NL_PLSA_2450', 'S0216.NL_PLSA_2447', 'S0216.NL_PLSA_2445', 'S0216.NL_PLSA_2439', 'S0216.NL_PLSA_2438', 'S0216.NL_PLSA_2437', 'S0216.NL_PLSA_2436', 'S0216.NL_PLSA_2434', 'S0216.NL_PLSA_2432', 'S0216.NL_PLSA_2430', 'S0216.NL_PLSA_2429', 'S0216.NL_PLSA_2441', 'S0217.NL_PLSA_2457', 'S0217.NL_PLSA_2480', 'S0217.NL_PLSA_2478', 'S0217.NL_PLSA_2477', 'S0217.NL_PLSA_2458', 'S0217.NL_PLSA_2471', 'S0217.NL_PLSA_2463', 'S0217.NL_PLSA_2466', 'S0217.NL_PLSA_2467', 'S0217.NL_PLSA_2468', 'S0218.INDIA_2510_PLS_1', 'S0218.INDIA_2524_PLS_1', 'S0218.INDIA_2500_PLS_1', 'S0218.INDIA_2518_PLS_1', 'S0218.INDIA_2519_PLS_1', 'S0218.INDIA_2520_PLS_1', 'S0218.INDIA_2521_PLS_1', 'S0218.INDIA_2522_PLS_1', 'S0218.INDIA_2499_PLS_1', 'S0225.INDIA_4144_PLS_1', 'S0226.INDIA_4150_PLS_1', 'S0226.INDIA_4151_PLS_1', 'S0226.NL_PLSA_2491', 'S0226.NL_PLSA_2490', 'S0226.NL_PLSA_2488', 'S0226.INDIA_4146_PLS_1', 'S0226.NL_PLSA_2492', 'S0227.INDIA_4154_PLS_1', 'S0235.INDI_157_PLS_1A', 'S0235.INDI_351_PLS_1A', 'S0238.INDIA_4271_PLS_1', 'S0256.LCRA_1740_PLS_01', 'S0256.LCRA_1741_PLS_01', 'S0263.INDIA_4357_PLS_1', 'S0263.INDIA_4359_PLS_1', 'S0263.INDIA_4361_PLS_1', 'S0281.INDIA_3134_PLS_1', 'S0282.INDIA_3156_PLS_1', 'S0283.INDIA_3187_PLS_1', 'S0285.INDIA_4639_PLS_1', 'S0285.INDIA_4645_PLS_1', 'S0287.INDIA_4756_PLS_1', 'S0287.INDIA_4765_PLS_1', 'S0287.INDIA_4758_PLS_1', 'S0287.INDIA_4759_PLS_1', 'S0287.INDIA_4760_PLS_1', 'S0287.INDIA_4762_PLS_1', 'S0287.INDIA_4763_PLS_1', 'S0287.INDIA_4776_PLS_1', 'S0287.INDIA_4777_PLS_1', 'S0287.INDIA_4774_PLS_1', 'S0287.INDIA_4771_PLS_1', 'S0287.INDIA_4770_PLS_1', 'S0287.INDIA_4769_PLS_1', 'S0287.INDIA_4768_PLS_1', 'S0287.INDIA_4775_PLS_1', 'S0290.INDIA_4617_PLS_1', 'S0290.INDIA_4582_PLS_1', 'S0290.INDIA_4584_PLS_1', 'S0290.INDIA_4586_PLS_1', 'S0290.INDIA_4588_PLS_1', 'S0290.INDIA_4589_PLS_1', 'S0290.INDIA_4590_PLS_1', 'S0290.INDIA_4587_PLS_1', 'S0290.INDIA_4616_PLS_1', 'S0290.INDIA_4596_PLS_1', 'S0290.INDIA_4595_PLS_1', 'S0290.INDIA_4594_PLS_1', 'S0290.INDIA_4593_PLS_1', 'S0290.INDIA_4612_PLS_1']\n",
      "(352, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bk/ghnkt7w957q5kkxzfzy0_wj40000gn/T/ipykernel_25250/2533493973.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[\"Cancer Status\"] = data[\"Cancer Status\"].replace([\"Healthy\", \"No\", \"Normal\"], 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((352, 3), (352,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = np.load(FILE_PATH+FILE[1], allow_pickle=True)\n",
    "data = pd.read_csv(FILE_PATH)\n",
    "data.to_pickle(FILE_PATH + \".pkl\")\n",
    "\n",
    "\n",
    "data[\"Cancer Status\"] = data[\"Cancer Status\"].replace([\"Cancer\", \"Yes\"], 1)\n",
    "data[\"Cancer Status\"] = data[\"Cancer Status\"].replace([\"Healthy\", \"No\", \"Normal\"], 0)\n",
    "data[\"Sample\"] = data[\"Experiment\"] + \".\" + data[\"Sample\"]\n",
    "print(data)\n",
    "\n",
    "\n",
    "train = []\n",
    "with open('/Users/baiyuxin/Desktop/JHU/NDD/Cancer/MIGHT_TEST_v0.6.1/Cohort1.samples.txt', \"r\") as mydata:\n",
    "       print(mydata)\n",
    "       for line in mydata:\n",
    "        if line.startswith(\"Experiment\"):\n",
    "            continue\n",
    "        data_i = [item.strip() for item in line.split(\" \")]\n",
    "        # print(data_i)\n",
    "        train.append(data_i[0] + \".\" + data_i[1])\n",
    "print(train)\n",
    "data = data[data[\"Sample\"].isin(train)]\n",
    "\n",
    "data.index = range(len(data))\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "\n",
    "columns_to_remove = [\n",
    "                \"Experiment\",\n",
    "                \"Run\",\n",
    "                \"Sample\",\n",
    "                \"Library\",\n",
    "                \"Cancer Status\",\n",
    "                \"Tumor type\",\n",
    "                \"MAF\",\n",
    "                \"Stage\",\n",
    "                \"P7\",\n",
    "                \"P7 primer\",\n",
    "                \"P7 Primer\",\n",
    "                \"Library volume\",\n",
    "                \"Library Volume\",\n",
    "                \"UIDs Used\",\n",
    "                \"Avg GC\",\n",
    "                \"Library volume (uL)\",\n",
    "                \"Total Reads\",\n",
    "                \"Total Alu\",\n",
    "            ]\n",
    "\n",
    "y = data[\"Cancer Status\"].to_numpy()\n",
    "features = data.loc[:, ~data.columns.isin(columns_to_remove)].columns.tolist()\n",
    "X = data.loc[:, features].to_numpy()\n",
    "X = np.nan_to_num(X)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38235294117647056\n",
      "0.37254901960784315\n",
      "0.38235294117647056\n",
      "0.38235294117647056\n",
      "0.37254901960784315\n",
      "0.38235294117647056\n",
      "0.38235294117647056\n",
      "0.38235294117647056\n",
      "0.38235294117647056\n",
      "0.37254901960784315\n",
      "0.38235294117647056\n",
      "0.37254901960784315\n",
      "0.37254901960784315\n",
      "0.37254901960784315\n",
      "0.38235294117647056\n",
      "0.37254901960784315\n",
      "0.38235294117647056\n",
      "0.37254901960784315\n",
      "0.37254901960784315\n",
      "0.38235294117647056\n"
     ]
    }
   ],
   "source": [
    "### Try 1k estimators (same parameters and data preprocessing with Sam's code)\n",
    "est = HonestForestClassifier(n_estimators=1000,\n",
    "# random_state=seed,\n",
    "honest_fraction=0.5,\n",
    "n_jobs=-1,\n",
    "bootstrap=True,\n",
    "stratify=True,\n",
    "max_samples=1.6,\n",
    "max_features=0.3\n",
    "# permute_per_tree=True,\n",
    ")\n",
    "S98 = []\n",
    "for i in range(20):\n",
    "    _, posterior_arr = build_hyppo_oob_forest(\n",
    "    est,\n",
    "    X,\n",
    "    y,\n",
    "    verbose=False,\n",
    "    )\n",
    "    sas98 = sensitivity_at_specificity(\n",
    "    y, posterior_arr, target_specificity=0.98\n",
    "    )\n",
    "    print(sas98)\n",
    "    S98.append(sas98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37254901960784315\n",
      "0.37254901960784315\n"
     ]
    }
   ],
   "source": [
    "### Try with 100k estimators\n",
    "est = HonestForestClassifier(n_estimators=100000,\n",
    "# random_state=seed,\n",
    "honest_fraction=0.5,\n",
    "n_jobs=-1,\n",
    "bootstrap=True,\n",
    "stratify=True,\n",
    "max_samples=1.6,\n",
    "max_features=0.3\n",
    "# permute_per_tree=True,\n",
    ")\n",
    "S98 = []\n",
    "for i in range(2):\n",
    "    _, posterior_arr = build_hyppo_oob_forest(\n",
    "    est,\n",
    "    X,\n",
    "    y,\n",
    "    verbose=False,\n",
    "    )\n",
    "    sas98 = sensitivity_at_specificity(\n",
    "    y, posterior_arr, target_specificity=0.98\n",
    "    )\n",
    "    print(sas98)\n",
    "    S98.append(sas98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37254901960784315,\n",
       " 0.37254901960784315,\n",
       " 0.37254901960784315,\n",
       " 0.37254901960784315,\n",
       " 0.37254901960784315]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try with 50k trees\n",
    "### Try with 100k estimators\n",
    "est = HonestForestClassifier(n_estimators=50000,\n",
    "# random_state=seed,\n",
    "honest_fraction=0.5,\n",
    "n_jobs=-1,\n",
    "bootstrap=True,\n",
    "stratify=True,\n",
    "max_samples=1.6,\n",
    "max_features=0.3\n",
    "# permute_per_tree=True,\n",
    ")\n",
    "S98 = []\n",
    "for i in range(2):\n",
    "    _, posterior_arr = build_hyppo_oob_forest(\n",
    "    est,\n",
    "    X,\n",
    "    y,\n",
    "    verbose=False,\n",
    "    )\n",
    "    sas98 = sensitivity_at_specificity(\n",
    "    y, posterior_arr, target_specificity=0.98\n",
    "    )\n",
    "    print(sas98)\n",
    "    S98.append(sas98)\n",
    "print(S98)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "might_bootstrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
